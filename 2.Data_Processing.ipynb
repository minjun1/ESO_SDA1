{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01fdb200-8d1e-4abe-a476-ea775157c798",
   "metadata": {},
   "source": [
    "# Data Processing for DAS H5 Files\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Load file paths from pickles (which list `.h5` files in GCS).\n",
    "2. Process, clip, and normalize the data.\n",
    "3. Reshape (crop) and combine event/noise sets.\n",
    "4. Save the processed arrays to `.npy` files for training/testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06b2a70e-a979-4540-9114-938120bb1362",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# notebook cell 1: imports\n",
    "from my_scripts.data_processing import main_pipeline\n",
    "\n",
    "# Optionally, if you need specific functions:\n",
    "# from data_processing import (\n",
    "#     load_file_paths,\n",
    "#     process_h5_files,\n",
    "#     ensure_uniformity_and_convert\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66274625-de94-44c9-8fd9-9b565a4eea0b",
   "metadata": {},
   "source": [
    "## Step 1: Set Up and Run the Pipeline\n",
    "\n",
    "In this example, we assume you already have pickle files listing:\n",
    "- `train_event_files.pkl`\n",
    "- `train_noise_files.pkl`\n",
    "- `test_event_files.pkl`\n",
    "- `test_noise_files.pkl`\n",
    "\n",
    "Adjust crop sizes, percentile, etc. as needed. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20fc517c-8e5e-4f9b-b135-1c6e6ee5405e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train_event_files.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m OUTPUT_DIR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_data\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Run the pipeline\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mmain_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_event_pkl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_noise_pkl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_event_pkl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_noise_pkl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcrop_size1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCROP_SIZE1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcrop_size2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCROP_SIZE2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclip_percentile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCLIP_PERCENTILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalization_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNORMALIZATION_RANGE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOUTPUT_DIR\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/embedding_DAS_earthquake/SDA1/data_processing.py:186\u001b[0m, in \u001b[0;36mmain_pipeline\u001b[0;34m(train_event_pkl, train_noise_pkl, test_event_pkl, test_noise_pkl, crop_size1, crop_size2, clip_percentile, normalization_range, out_dir)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03mFull data processing pipeline:\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m1. Load file paths from pickles.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03mNone. (Saves arrays to disk.)\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# 1. Load file paths\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m file_paths_train_event \u001b[38;5;241m=\u001b[39m \u001b[43mload_file_paths\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_event_pkl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m file_paths_train_noise \u001b[38;5;241m=\u001b[39m load_file_paths(train_noise_pkl)\n\u001b[1;32m    188\u001b[0m file_paths_test_event  \u001b[38;5;241m=\u001b[39m load_file_paths(test_event_pkl)\n",
      "File \u001b[0;32m~/embedding_DAS_earthquake/SDA1/data_processing.py:32\u001b[0m, in \u001b[0;36mload_file_paths\u001b[0;34m(pkl_filename)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_file_paths\u001b[39m(pkl_filename):\n\u001b[1;32m     19\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    Load file paths (list of strings) from a pickle file.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m        List of file paths\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpkl_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     33\u001b[0m         file_paths \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile paths loaded from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpkl_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train_event_files.pkl'"
     ]
    }
   ],
   "source": [
    "# notebook cell 2: define parameters\n",
    "train_event_pkl = 'train_event_files.pkl'\n",
    "train_noise_pkl = 'train_noise_files.pkl'\n",
    "test_event_pkl  = 'test_event_files.pkl'\n",
    "test_noise_pkl  = 'test_noise_files.pkl'\n",
    "\n",
    "CROP_SIZE1 = 288\n",
    "CROP_SIZE2 = 695\n",
    "CLIP_PERCENTILE = 85\n",
    "NORMALIZATION_RANGE = (-2, 2)\n",
    "OUTPUT_DIR = 'processed_data'\n",
    "\n",
    "# Run the pipeline\n",
    "main_pipeline(\n",
    "    train_event_pkl,\n",
    "    train_noise_pkl,\n",
    "    test_event_pkl,\n",
    "    test_noise_pkl,\n",
    "    crop_size1=CROP_SIZE1,\n",
    "    crop_size2=CROP_SIZE2,\n",
    "    clip_percentile=CLIP_PERCENTILE,\n",
    "    normalization_range=NORMALIZATION_RANGE,\n",
    "    out_dir=OUTPUT_DIR\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eb7f84-93dd-4ea5-8dc2-719a5bb76e84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-15.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-gpu.2-15:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
